### A Pluto.jl notebook ###
# v0.14.5

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# ‚ïî‚ïê‚ï° e9f41c8b-9400-4d2d-a2e0-db016b62b69c
begin
	using Pkg; Pkg.activate(".")
	using Plots, PlutoUI
end

# ‚ïî‚ïê‚ï° ae6ae8e9-8d53-4a5a-929f-e02fa3a10901
PlutoUI.TableOfContents(title = "Contenidos üíª")

# ‚ïî‚ïê‚ï° 3d577cb4-9418-45ee-a968-62f3b5287759
md"# Segundo d√≠a: Introducci√≥n al pensamiento computacional y algor√≠tmico
## Minimizaci√≥n en una variable

considere la siguiente funci√≥n:"

# ‚ïî‚ïê‚ï° 8531e010-baf8-11eb-2b37-6be3738f5f8e
f(x) = (x^2 - 1)*(x-2)

# ‚ïî‚ïê‚ï° cef9b84e-00e6-4c8f-9694-03b2a8557eb4
plot(f, -1.5:0.1:2.7) 

# ‚ïî‚ïê‚ï° ee1fe570-b26e-4adc-a5f7-4fbc77c5a29f
md"Esta funci√≥n, al ser un polinomio, es f√°cil encontrar sus m√≠nimos locales y m√≠nimo global utilizando el c√°lculo. No obstante, podemos usarlo de modelo de alguna funci√≥n con caracter√≠sticas similares pero forma anal√≠tica mucho m√°s compleja, o potencialmente una funci√≥n emp√≠rica para la cual solamente tengamos los datos obtenidos de observaciones.

### Encontrando un intervalo inicial

El primer paso para encontrar el m√≠nimo local de una funci√≥n general unimodal (que tiene solo un m√≠nimo) es encontrar un intervalo $[a,b]$ tal que el punto minimizante $x_{\text{m√≠n}}$ est√© en $(a,b)$. En caso de funciones que no son unimodales, podemos tener el riesgo de encontrar un m√≠nimo local que no sea un m√≠nimo global. 

Ahora, incluso en funciones unimodales, debemos encontrar un intervalo $[a,b]$ v√°lido como descrito anteriormente pues puede ser el caso que comencemos la b√∫squeda del m√≠nimo erroneamente con un intervalo $[a,b]$ tal que $x < a < b$ o $a < b < x$ para los $x$ verificados.

Un m√©todo para poder encontrar un bracket inicial con el cual trabajar es el siguiente:

1. Comenzamos con un punto inicial cualquiera, $x$. Entre m√°s cerca de $x_{\text{m√≠n}}$ estemos, mejor (aunque a veces no hay forma de saber).
2. Dicho punto y su imagen: $x, f(x)$, van a ser respectivamente la primera propuesta de $a$ (en $[a,b]$) y su imagen, $f(a)$.
3. Tomamos un peque√±o paso a la izquierda (digamos de tama√±o $s$) para obtener la primera propuesta de $b$ y su imagen, $f(b)$.
4. Puede ser el caso que nuestro paso de tama√±o $s$ nos haya llevado a un punto m√°s alto y no uno m√°s bajo (¬°recordemos que deseamos minimizar la funci√≥n!). En ese caso, debemos intercambiar los roles de $a$ con $b$ y los de sus im√°genes. 
5. Una vez que sepamos que estamos en direcci√≥n de descenso, debemos seguir tomando pasos (si deseamos, siempre de tama√±o $s$ o, en general, de un nuevo tama√±o $k \times s$ cada vez). 
6. El algoritmo de detendr√° en el momento que la imagen del punto actualizado, $x_{\text{prev}} + k\times s$, sea mayor a la imagen de $x_{\text{prev}}$ ya que, bajo la premisa de que ya est√°bamos en direcci√≥n de descenso, esto solo puede significar que hemos saltado sobre el punto m√≠nimo y comenzado a subir. 
7. En conclusi√≥n de ello, los valores actuales en esa iteraci√≥n para $a$ y $b$ ser√°n los que garanticen que $x_{\text{m√≠n}} \in [a,b]$.

Implementemos el algoritmo a continuaci√≥n:
"

# ‚ïî‚ïê‚ï° 69cdb427-8822-4cd6-9be7-ee4593ad7c21
function bracket_minimum(f, x=0; s=1e-2, k=2.0)
	a, ya = x, f(x)
	b, yb = a + s, f(a + s)
	if yb > ya
		a, b = b, a
		ya, yb = yb, ya
		s = -s
	end
	while true
		c, yc = b + s, f(b + s)
		if yc > yb
			return a < c ? (a, c) : (c, a)
		end
		a, ya, b, yb = b, yb, c, yc
		s *= k
	end
end		

# ‚ïî‚ïê‚ï° 61db35aa-8511-4241-8824-a0f3d9794ca8
md"Los par√°metros $s$ y $k$ del algoritmo son llamados *hiperpar√°metros* ya que, m√°s que par√°metros que nos definen un comportamiento espec√≠fico de nuestro algoritmo, son par√°metros que tienen que ser decididos en torno al tipo de funci√≥n, $f$, que estemos tratando, y tendr√°n mejor o peor rendimiento dependiendo de las cualidades de $f$.

Podemos ilustrar como la elecci√≥n del intervalo con el m√≠nimo depende de √©stos dos para nuestra funci√≥n actual:"

# ‚ïî‚ïê‚ï° f92fdc19-1d54-492e-9690-f8a0bcadb425
md"s = $(@bind s Slider(0.02:0.02:1, show_value = true, default = 0.01))

   k = $(@bind k Slider(1.1:0.1:4.0, show_value = true, default = 2.0))"

# ‚ïî‚ïê‚ï° 976e8f79-5b81-4ad9-af58-e84942ed7db5
let (a, b) = bracket_minimum(f, 1; s=s, k=k)
	plot(f, -1.5:0.1:2.7, label = "f(x)")
	plot!([(a, 0), (b, 0)], seriestype = :scatter, 
		  label = "intervalo encontrado", markershape = :vline,
	      markersize = 6)
end

# ‚ïî‚ïê‚ï° 02f5dd5e-c221-4e21-98d4-958e8d7ac139
md"Una vez encontrado este intervalo, la siguiente tarea es encontrar el punto m√≠nimo de la funci√≥n, $x_{\text{m√≠n}}$. Esto puede ser logrado por diversos algoritmos, pero en este caso deduciremos uno llamado **b√∫squeda de ajuste cuadr√°tico** (o quadratic fit search).

### B√∫squeda de ajuste cuadr√°tico

Este deriva su nombre de su estrategia: Ajustar a $f$ en $[a,b]$ a una funci√≥n cuadr√°tica, bajo la premisa que muchas funciones se comportan similar a polinomios cuadr√°ticos en vecindad de un m√≠nimo. Esto quiere decir que, entre m√°s peque√±o y cercano sea $[a,b]$ del punto m√≠nimo, mejor rendimiento tendremos.

Comencemos con la deducci√≥n:
1. Para fines de notaci√≥n, llamemos a lo que previamente era el intervalo $[a,b]$ a $[a,c]$ ya que vamos a querer llevar rastro de un punto interior, que llamaremos $b$, tal que: $a < b < c$ para servir como propuesta de $x_{\text{m√≠n}}$, la cual iremos actualizando.
2. Definamos nuestra funci√≥n de ajuste cuadr√°tico: $q(x) = p_1 + p_2x + p_3x^2$
3. Podemos hacer que $q(x)$ defina una cuadr√°tica que pase por todos nuestros puntos conocidos: $(a, f(a)), (b, f(b)), (c, f(c))$ al encontrar los coeficientes $p_i$ adecuados. Como notaci√≥n los $f(k)$ ahora ser√° denotado $y_k$ para $k = a,b,c$. Expresamos las relaciones resultantes:

   - $y_a = p_1 + p_2a + p_3a^2$
   - $y_b = p_1 + p_2b + p_3b^2$
   - $y_c = p_1 + p_2c + p_3c^2$

Esto se puede resumir mejor en la siguiente ecuaci√≥n matricial:

$\begin{bmatrix}
y_a \\ y_b \\ y_c
\end{bmatrix}
= \begin{bmatrix}
1 & a & a^2 \\ 1 & b & b^2  \\ 1 & c & c^2 
\end{bmatrix} 
\begin{bmatrix}
p_1 \\ p_2 \\ p_3 
\end{bmatrix}$

Para resolver esta ecuaci√≥n basta con encontrar la matriz inversa, esto ya que podemos calcular que el determinante de esta matriz es: $-(a-b)(a-c)(b-c)$ (¬°Verif√≠quenlo!) y √©sta expresi√≥n solamente puede ser cero si al menos alguno de los tres puntos es el mismo. 

Asumiendo entonces que la matriz es no singular, tenemos:

$\begin{bmatrix}
p_1 \\ p_2 \\ p_3 
\end{bmatrix}
= \begin{bmatrix}
1 & a & a^2 \\ 1 & b & b^2  \\ 1 & c & c^2 
\end{bmatrix}^{-1} 
\begin{bmatrix}
y_a \\ y_b \\ y_c
\end{bmatrix}$

la cual puede ser resuelta con su m√©todo favorito. Ya que tenemos el determinante, podemos utlizar la [**regla de Cramer**](https://en.wikipedia.org/wiki/Cramer%27s_rule) para obtener que nuestro polinomio cuadr√°tico ajustado es:

$q(x) = y_a\frac{(x-b)(x-c)}{(a-b)(a-c)} + y_b\frac{(x-a)(x-c)}{(b-a)(b-c)} + y_c\frac{(x-b)(x-a)}{(c-b)(c-a)}$

Calculando $q'(x)$, la derivada de $q(x)$, e igualando a cero podemos obtener el m√≠nimo global de la cuadr√°tica, el cual sirve como una buena propuesta para el punto m√≠nimo $x_{\text{m√≠n}}$ de nuestra funci√≥n. Estos comenzar√°n a coincidir a medida el intervalo sea m√°s peque√±o y $f$ parezca cada vez m√°s una cuadr√°tica en el intervalo $[a,c]$. 

Motivados por ello, debemos proponer una forma de actualizar el intervalo $[a,c]$ utilizando el valor de la nueva propuesta √≥ptima:

$x^* = \frac{1}{2} \frac{y_a(b^2-c^2) + y_b(c^2-a^2) + y_c(a^2-b^2)}{y_a(b-c) + y_b(c-a) + y_c(a-b)}$

que vendr√° a substituir nuestra propuesta inicial $b$. Bosquejemos el algoritmo a continuaci√≥n:

1. Definamos un n√∫mero $n$ de iteraciones m√°ximas que deseamos hacer. 

   Este $n$ podr√≠a estar controlado por algo m√°s sofisticado como reconocer que nuestro algoritmo generar√° puntos en una sucesi√≥n convergente la cual tambi√©n es de Cauchy y por ende podemos llevar rastro de la diferencia entre puntos propuestos consecutivos y saber que el resultado se ir√° haciendo m√°s pequeno. Entonces, la condici√≥n de paro puede ser una tolerancia $\epsilon$ bajo la cual nos baste que yazca la diferencia de nuestras propuestas.


2. De nuestros tres puntos iniciales $a,b,c$ ($a$ y $c$ siendo los l√≠mites del intervalo que contiene el m√≠nimo y $b$ un punto interior arbitrario), podemos calcular $x^*$ en base a la f√≥rmula deducida anteriormente.


3. El $x^*$ encontrado puede estar a la izquierda o a la derecha de $b$ (pues, como ser√° demostrable luego, como nuestro algoritmo converge, si $x^*$ coincide con $b$, quiere decir que $b$ ya era nuestro punto √≥ptimo desde el inicio y podemos retornarlo).

   Independiente de d√≥nde caiga, tendremos una estrategia general de actualizaci√≥n. Consideremos sin p√©rdida de generalidad que cae adelante de $b$. Entonces: $x^* \in (b, c]$ (en el caso que haya ca√≠do a la derecha, $x^* \in [a, b)$).


4. Verificamos si la imagen, $f(x^*)$, de nuestra propuesta es mayor o menor que $y_b$:
   - Si es mayor, quiere decir que nos hemos saltado a $x_{\text{m√≠n}}$ y podemos utilizar a $x^*$ como nuestro nuevo $c$ para achicar el intervalo.
   - Si es menor, quiere decir que $x_{\text{m√≠n}}$ a√∫n no ha sido alcanzado (est√° por enfrente tanto de $x^*$ como de $b$) y entonces podemos descartar a $a$ como intervalo interior en favor de $b$.


5. Realizada esta actualizaci√≥n del intervalo, podremos repetir el paso 2 para encontrar una nueva propuesta y repetir los pasos 3 y 4 hasta cumplir nuestra condici√≥n de paro.


6. Una vez detenido el algoritmo, podemos devolver los √∫ltimos valores de los tres n√∫meros: $a,b,c$ donde $b$ ser√° nuestra mejor propuesta para $x_{\text{m√≠n}}$ mientras que $[a,c]$ seguir√° siendo un intervalo que garantice contener a $x_{\text{m√≠n}}$.

Prosigamos a implementarlo:
"

# ‚ïî‚ïê‚ï° bc48831c-1c0b-4091-b8b5-07384de3039b
function quadratic_fit_search(f, a, b, c, n)
	ya, yb, yc = f(a), f(b), f(c)
	for i in 1:n
		x = 0.5*(ya*(b^2-c^2)+yb*(c^2-a^2)+yc*(a^2-b^2)) /
		(ya*(b-c)
		+yb*(c-a)
		+yc*(a-b))
		yx = f(x)
		if x > b
			if yx > yb
				c, yc = x, yx
			else
				a, ya, b, yb = b, yb, x, yx
			end
		elseif x < b
			if yx > yb
				a, ya = x, yx
			else
				c, yc, b, yb = b, yb, x, yx
			end
		end
	end
	return (a, b, c)
end

# ‚ïî‚ïê‚ï° ad161424-fcd7-4d3e-8d65-c1d9be8ab92f
md"Quisieramos poder juntar ambos algoritmos: El de la b√∫squeda del intervalo con m√≠nimo y la b√∫squeda de ajuste cuadr√°tico. Para ello, aprovechamos el **multiple dispatch** de Julia para definir otro m√©todo de la funci√≥n `quadratic_fit_search` que esta vez solo tome a $f$, a un punto inicial $x$ para la b√∫squeda del intervalo y el n√∫mero de iteraciones $n$.

Para ello, decidimos que nuestra propuesta inicial, $b \in [a,b]$ siempre ser√° el punto medio del intervalo. Prosegimos a la implementaci√≥n: "

# ‚ïî‚ïê‚ï° 701dbc97-ad20-4a22-9dfc-8bfc2e4b806f
function quadratic_fit_search(f, x, n)
	a, c = bracket_minimum(f, x)
	b = (a+c)/2
	quadratic_fit_search(f, a, b, c, n)
end

# ‚ïî‚ïê‚ï° 87292b27-8aef-495c-86f9-fca65993c382
md"Exploremos ahora c√≥mo la optimizaci√≥n de nuestra funci√≥n $f$ depende de la elecci√≥n de $x$ inicial y del n√∫mero de iteraciones."

# ‚ïî‚ïê‚ï° b4cd0cc3-bafe-45ab-b549-8a1d1e471195
√≥ptimo_real = (2+sqrt(7))/3

# ‚ïî‚ïê‚ï° 3475b4a0-d3b9-48c3-8cd5-70afad0e0aae
md"x = $(@bind x Slider(0.5:0.1:3, show_value = true))

   n = $(@bind n Slider(1:1:10, show_value = true))"

# ‚ïî‚ïê‚ï° 744e4de6-d4fe-4778-9291-ac8b66c887e4
bracket_minimum(f, x)

# ‚ïî‚ïê‚ï° 269f3e2f-df18-42a6-85de-1bfa9f84b358
a_opt, x_opt, c_opt = quadratic_fit_search(f, x, n)

# ‚ïî‚ïê‚ï° f1fcf108-bd8f-44d8-b28c-00228db3d092
x_opt

# ‚ïî‚ïê‚ï° 71722f4d-e8a9-4758-ab1d-477e5bf730c3
√≥ptimo_real - x_opt

# ‚ïî‚ïê‚ï° 87160279-33c2-40ca-908b-37dad0785d80
mean_error = sum(√≥ptimo_real .- (x -> quadratic_fit_search(f, x, n)[2]).(0.5:0.1:3))/length(0.5:0.1:3)

# ‚ïî‚ïê‚ï° bd2dca3a-dce7-4431-a5b3-a736b89d5493
begin
	plot(f, 0.5:0.01:2.2, label = "f(x)") 
	plot!([(√≥ptimo_real, f(√≥ptimo_real))], seriestype = :scatter, label = "√≥ptimo real")
	plot!([(x_opt, f(x_opt))], seriestype = :scatter, label = "punto predicho")
	plot!([(a_opt, -0.65), (c_opt, -0.65)], seriestype = :scatter, 
		  label = "intervalo final", markershape = :vline,
	      markersize = 10)
end

# ‚ïî‚ïê‚ï° ec124952-0a0d-4d24-b0b7-556945dea6f7
md"
## Minimizaci√≥n multivariable

El tema de minimizaci√≥n en una variable es much√≠simo m√°s extenso de lo que discutimos. Esto significa por supuesto que veremos muy por encima la minimizaci√≥n multivariable, pero el prop√≥sito de presentar al menos un m√©todo es que sea m√°s evidente la importancia de construir sobre fundamentos rigurosos hacia m√©todos que son utilizados en algoritmos muy importantes de la industria y frontera de la ciencia.

El algoritmo que visitaremos se llama **line search** y es parte de una familia de algoritmos utilizados para encontrar una propuesta de un punto m√°s cercano al m√≠nimo de alguna funci√≥n $f: \mathbb{R}^n \rightarrow \mathbb{R}$ tras ya conocer una *direcci√≥n √≥ptima de descenso* (la cual no discutiremos c√≥mo encontrar, pues involucra otra familia de m√©todos llamados descenso del gradiente, la cual no podremos discutir por tiempo).

Consideremos la siguiente funci√≥n:" 

# ‚ïî‚ïê‚ï° 1e067699-f725-4f45-9ba4-fafb315ee8cd
g(x, y) = 2*x^2 + y^2

# ‚ïî‚ïê‚ï° f9434958-f02d-484d-a54c-1039f5f91949
begin
	xs = as = collect(-1:0.1:1.0)
	x_grid = [x for x = xs for y = as]
	a_grid = [y for x = xs for y = as]
	plot(x_grid, a_grid, g.(x_grid, a_grid), st = :surface, xlabel = "longer xlabel", ylabel = "longer ylabel", zlabel = "longer zlabel")
end

# ‚ïî‚ïê‚ï° 1a10f9e8-044d-4b48-8887-854a029f25d4
md"
Imaginemos que conocemos un vector $\mathbf{d} \in \mathbb{R}^2$ que define una direcci√≥n de desenco desde un punto inicial, $\mathbf{x_0}\in \mathbb{R}^2$. Esta direcci√≥n de descenso es necesaria de saber similar a c√≥mo anteriormente ocupamos verificar en `bracket_minimum` que nuestros pasos `s` no nos llevaran a un valor mayor y por ende m√°s lejos del m√≠nimo.

Este valor de $\mathbf{d}$ es posible e intuitivo de definir como la direcci√≥n normalizada opuesta al gradiente (ya que el gradiente es la direcci√≥n del cambio m√°s inclinado positivo). Por ahora, lo dejaremos fijo a un valor y nos preocuparemos por bosquejar el algoritmo b√°sico de *line search*:

1. Conociendo un punto inicial $\mathbf{x_0}$ y la siguiente direcci√≥n de descenso $\mathbf{d}$, podemos estudiar la **secci√≥n de la func√≠√≥n** $f$ definida por:

   $f_{\mathbf{d}} : \alpha \longmapsto f(\mathbf{x} + \alpha \mathbf{d})$

   Es decir, aumentar la preimagen de f a lo largo de la direcci√≥n $\mathbf{d}$. Como esta nueva funci√≥n es de una dimensi√≥n, podemos encontrar el $\alpha_{\text{m√≠n}}$ con los m√©todos de una variable.


2. Elegimos usar `quadratic_fit_search` junto con `bracket_minimum` para optimizar la funci√≥n $f_\mathbf{d}$, aunque pudo haber sido cualquier otro m√©todo de optimizaci√≥n de una variable.


3. Retornamos como propuesta a un valor m√°s cercano a $\mathbf{x}_\text{m√≠n}$ al valor avanzado por el $\alpha$ \optimo: 

$\mathbf{x_1} = \mathbf{x_0} + \alpha_\text{m√≠n} \mathbf{d}$

El algoritmo se espera iterar subsecuentes veces, con diferentes valores de $\mathbf{d}$ cada vez, y tomando como valores iniciales los \mathbf{x_k} que vayamos generando en las iteraciones anteriores hasta converger al m√≠nimo de la $f$ multivariable.

Implementemos `line_search`:
"

# ‚ïî‚ïê‚ï° 6058cd96-9a01-4e2e-a532-88ff48fab5ff
function line_search(f, x, d)
	objective = Œ± -> f(x + Œ±*d...)
	_, Œ±, _ = quadratic_fit_search(objective, x'*d, 5)
	return x + Œ±*d
end

# ‚ïî‚ïê‚ï° 7ff4af40-f4d5-41c1-88e6-6add4870451d
md"Podemos verlo en acci√≥n en las siguientes celdas, notando que debemos a√∫n colocar *'a mano'* el vector direccional:"

# ‚ïî‚ïê‚ï° ed53175a-a94f-4149-8974-91c9f6699943
x‚ÇÄ = [1,-1]

# ‚ïî‚ïê‚ï° 558c4f2b-a484-4d3e-a1a1-f1cd4e6a035f
x‚ÇÅ = line_search(g, x‚ÇÄ, [1/sqrt(2), 1/sqrt(2)])

# ‚ïî‚ïê‚ï° b6c24326-fdd0-423b-8009-3dacad259ab5
x‚ÇÇ = line_search(g, x‚ÇÅ, [1/sqrt(2), -1/sqrt(2)])

# ‚ïî‚ïê‚ï° 7684c089-d99d-4221-ad9b-ae25c13f2708
x‚ÇÉ = line_search(g, x‚ÇÇ, [1/sqrt(2), 1/sqrt(2)])

# ‚ïî‚ïê‚ï° fec3f7de-07b0-401d-ba88-98aa47c398e4
line_search(g, x‚ÇÉ, [1/sqrt(2), -1/sqrt(2)])

# ‚ïî‚ïê‚ï° c33963ba-f967-46b4-915b-22b4492f610e
md"
## Tarea üò†

**Intrucciones:** Debe hacer AL MENOS uno de los siguientes ejercicios, pero se recomienda realizarlos todos para poder practicar los conceptos vistos.

1. Verificar la deducci√≥n del punto $x^*$ en el algoritmo de la b√∫squeda de ajuste cuadr√°tico con detalle.


2. Demostrar que en el algoritmo de la b√∫squeda de ajuste cuadr√°tico, nuestra sucesi√≥n de propuestas: $\{x^*_k\}_{k=1}^\infty$ realmente converge al m√≠nimo local $x_{\text{m√≠n}}$. *Pista:* pueden utilizar el [teorema del s√°ndwich](https://en.wikipedia.org/wiki/Squeeze_theorem) o probar que es una [sucesi√≥n de Cauchy](https://en.wikipedia.org/wiki/Cauchy_sequence).


3. Implementar una funci√≥n `quadratic_fit_search(f, x, œµ = 1e-3)` que contin√∫e iterando sobre el quatratic search hasta que dos propuestas consecutivas tengan una diferencia menor a `œµ`. note que `œµ` es un par√°metro opcional. 

   De esta manera podremos llamar `quadratic_fit_search(f, x)` para alg√∫n $x$ que creamos que est√° cerca del m√≠nimo real y el algoritmo se encargue del resto sin preocuparnos de especificar todos los hiperpar√°metros. **Nota:** Tambi√©n es v√°lido poner al resto de hiperpar√°metros como opcionales.

4. El `line_search` implementado es el m√°s b√°sico de toda la familia de m√©todos de line search. Dise√±emos uno ligeramente m√°s complicado al reconocer que el `line_search` normal puede ser costoso de ejecutar ya que en cada paso progresivo hacia el m√≠nimo, debemos ejecutar una optimizaci√≥n completa de `quadratic_fit_search`. 

   Esto puede no ser necesario. Basta con que la funci√≥n $f$ decrezca *lo suficiente*. ¬øCu√°nto es suficiente? Consideremos la siguiente condici√≥n:

   $f(\mathbf{x_{k+1}}) \leq f(\mathbf{x_{k}}) + \beta \alpha \nabla_{\mathbf{d_{k}}} f(\mathbf{x_{k}})$ 

   Un line search que se detiene cuando la condici√≥n anterior se cumple se conoce como *line search aproximado con condici√≥n de descenso suficiente* o *backtracking line search* y tiene como intuici√≥n subyacente que: $\nabla_{\mathbf{d_{k}}} f(\mathbf{x_{k}})$ es la pendiente inmediata de $f$ en el punto $\mathbf{x_{k}}$ y en direcci√≥n $\mathbf{d_{k}}$ y √©sta define un descenso que har√≠amos a primer orden de aproximaci√≥n, lo cual deber√≠a ser suficiente para detener la iteraci√≥n. 

   No obstante, tenemos un hiperpar√°metro $\beta$ que controla cu√°nto porcentaje de ese descenso de primer orden consideramos suficiente. Por ejemplo, si $\beta = 0.5$, estar√≠amos considerando que la mitad de lo que el gradiente los dice que es suficiente descender es ya suficiente para nosotros y en caso que $\beta = 0$, cualquier descenso es suficiente.

   Implementa aqu√≠ el algoritmo de `backtracking_line_search(f, ‚àáf, x, d, Œ±, Œ≤)` que con cualquier t√©cnica de minimizaci√≥n de una dimensi√≥n logre minimizar una funci√≥n unimodal multivariada m√°s eficientemente que nuestra implementaci√≥n de `line_search`.
"

# ‚ïî‚ïê‚ï° Cell order:
# ‚ï†‚ïêe9f41c8b-9400-4d2d-a2e0-db016b62b69c
# ‚ïü‚îÄae6ae8e9-8d53-4a5a-929f-e02fa3a10901
# ‚ïü‚îÄ3d577cb4-9418-45ee-a968-62f3b5287759
# ‚ï†‚ïê8531e010-baf8-11eb-2b37-6be3738f5f8e
# ‚ï†‚ïêcef9b84e-00e6-4c8f-9694-03b2a8557eb4
# ‚ïü‚îÄee1fe570-b26e-4adc-a5f7-4fbc77c5a29f
# ‚ï†‚ïê69cdb427-8822-4cd6-9be7-ee4593ad7c21
# ‚ïü‚îÄ61db35aa-8511-4241-8824-a0f3d9794ca8
# ‚ïü‚îÄf92fdc19-1d54-492e-9690-f8a0bcadb425
# ‚ïü‚îÄ976e8f79-5b81-4ad9-af58-e84942ed7db5
# ‚ïü‚îÄ02f5dd5e-c221-4e21-98d4-958e8d7ac139
# ‚ï†‚ïêbc48831c-1c0b-4091-b8b5-07384de3039b
# ‚ïü‚îÄad161424-fcd7-4d3e-8d65-c1d9be8ab92f
# ‚ï†‚ïê701dbc97-ad20-4a22-9dfc-8bfc2e4b806f
# ‚ïü‚îÄ87292b27-8aef-495c-86f9-fca65993c382
# ‚ï†‚ïê744e4de6-d4fe-4778-9291-ac8b66c887e4
# ‚ï†‚ïê269f3e2f-df18-42a6-85de-1bfa9f84b358
# ‚ï†‚ïêf1fcf108-bd8f-44d8-b28c-00228db3d092
# ‚ï†‚ïêb4cd0cc3-bafe-45ab-b549-8a1d1e471195
# ‚ï†‚ïê71722f4d-e8a9-4758-ab1d-477e5bf730c3
# ‚ïü‚îÄ87160279-33c2-40ca-908b-37dad0785d80
# ‚ïü‚îÄ3475b4a0-d3b9-48c3-8cd5-70afad0e0aae
# ‚ïü‚îÄbd2dca3a-dce7-4431-a5b3-a736b89d5493
# ‚ïü‚îÄec124952-0a0d-4d24-b0b7-556945dea6f7
# ‚ï†‚ïê1e067699-f725-4f45-9ba4-fafb315ee8cd
# ‚ïü‚îÄf9434958-f02d-484d-a54c-1039f5f91949
# ‚ïü‚îÄ1a10f9e8-044d-4b48-8887-854a029f25d4
# ‚ï†‚ïê6058cd96-9a01-4e2e-a532-88ff48fab5ff
# ‚ïü‚îÄ7ff4af40-f4d5-41c1-88e6-6add4870451d
# ‚ï†‚ïêed53175a-a94f-4149-8974-91c9f6699943
# ‚ï†‚ïê558c4f2b-a484-4d3e-a1a1-f1cd4e6a035f
# ‚ï†‚ïêb6c24326-fdd0-423b-8009-3dacad259ab5
# ‚ï†‚ïê7684c089-d99d-4221-ad9b-ae25c13f2708
# ‚ï†‚ïêfec3f7de-07b0-401d-ba88-98aa47c398e4
# ‚ïü‚îÄc33963ba-f967-46b4-915b-22b4492f610e
